# -*- coding: utf-8 -*-
"""LibrispeechDataLoader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVVEsrSK5XtOxv3L3ZQoFAx5VjVDOLC-

# MMAI Lab KAIST Individual Study Project 2024

>Make Librispeech dataset dataloader => Output tensor of audio and text label.

## Citation of Librispeech Dataset
>V. Panayotov, G. Chen, D. Povey and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), South Brisbane, QLD, Australia, 2015, pp. 5206-5210, doi: 10.1109/ICASSP.2015.7178964.
>>Abstract: This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.URL:Â https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178964&isnumber=7177909

>>Data Source: https://www.openslr.org/12
"""

from google.colab import files
import os
import torch
import torchaudio
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from torchaudio.transforms import Resample
import numpy as np

class LibriSpeechData(Dataset):
    def __init__(self, file_path, transform=None) -> None:
        super().__init__()
        self.data_path = file_path
        self.transform = transform
        self.audio_paths, self.text_labels = self._load_data()

    def _load_data(self):
        audio_paths = []
        text_labels = []

        for speaker in os.listdir(self.data_path):
            speaker_path = os.path.join(self.data_path, speaker)
            for chapter in os.listdir(speaker_path):
                chapter_path = os.path.join(speaker_path, chapter)
                transcript_path = None
                chapter_audio_files = []

                # Identify transcript file and collect all audio file paths
                for data_file in os.listdir(chapter_path):
                    if data_file.endswith('.flac'):
                        audio_path = os.path.join(chapter_path, data_file)
                        chapter_audio_files.append(audio_path)
                    elif data_file.endswith('.txt'):
                        transcript_path = os.path.join(chapter_path, data_file)

                # If transcript file exists, associate each line with an audio file
                if transcript_path:
                    with open(transcript_path, 'r') as transcript_file:
                        transcripts = transcript_file.readlines()

                    # Naming and ordering in transcript are consistent
                    for audio_path, transcript in zip(sorted(chapter_audio_files), transcripts):
                        audio_paths.append(audio_path)
                        text_labels.append(transcript.strip())

        return audio_paths, text_labels

    def __len__(self):
        return len(self.audio_paths)

    def __getitem__(self, idx):
        audio_path = self.audio_paths[idx]
        waveform, sample_rate = torchaudio.load(audio_path)
        label = self.text_labels[idx]

        # if sample_rate != 16000:
        #     resampler = Resample(orig_freq=sample_rate, new_freq=16000)
        #     waveform = resampler(waveform)

        if self.transform:
            waveform = self.transform(waveform)

        return waveform, sample_rate, label

def collate_fn(batch):
    waveforms, sample_rates, labels = zip(*batch)
    target_sample_rate = 16000

    # Resampling and find the max waveform length
    max_len = 0
    resampled_waveforms = []
    for waveform, sample_rate in zip(waveforms, sample_rates):
        resampled_waveform = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)
        max_len = max(max_len, resampled_waveform.shape[1])
        resampled_waveforms.append(resampled_waveform)

    # Padding the short waveforms to have the same length as the longest waveform
    padded_waveforms = []
    for waveform in resampled_waveforms:
        padded_waveform = torch.nn.functional.pad(waveform, (0, max_len - waveform.shape[1]))
        padded_waveforms.append(padded_waveform)

    # Stacking padded waveforms
    waveforms_tensor = torch.stack(padded_waveforms)

    return waveforms_tensor, labels

# DataLoader Parameters
BATCH_SIZE = 32
NUM_WORKERS = 2

def paulSpeechDataLoader(data_path, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, transform=None):
  data = LibriSpeechData(data_path, transform=transform)
  dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)

  return dataloader

# with open('/content/drive/MyDrive/LibriSpeech/dev-clean/1272/128104/1272-128104.trans.txt', 'r') as f:
#   print(f.readlines())

data_path = "/content/drive/MyDrive/LibriSpeech/dev-clean"

dataloader = paulSpeechDataLoader(data_path=data_path, batch_size=BATCH_SIZE)

for idx, (audio, text_label) in enumerate(dataloader):
  print(f"Batch {idx}: Audio Tensor Shape: {audio.shape}, Labels: {text_label}")

def write2txt(text_label):
  with open('/content/output_trans.txt', 'w') as transcript:
    for text_line in list(text_label):
      transcript.write(text_line + "\n")

# Exporting sample audio with text labels
for idx, (audio, text_label) in enumerate(dataloader):
  concatenated_audio = torch.cat([audio[i] for i in range(audio.size(0))], dim=1)
  torchaudio.save('/content/output_audio.wav', concatenated_audio, sample_rate=16000)
  write2txt(text_label)

  print(audio)
  print(text_label)
  break

# sorted(os.listdir('/content/drive/MyDrive/LibriSpeech/dev-clean/1272/128104'))

# with open('/content/output_trans.txt', 'w') as transcript:
#   for text_line in list(text_label):
#     transcript.write(text_line + "\n")

